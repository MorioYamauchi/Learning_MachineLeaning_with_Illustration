{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習モデル\n",
    "機械学習の問題の多くは，ある関数をデータから近似するという問題として定式化（パターン化）される．2章では，関数を近似するために用いる様々なモデルを紹介する．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 線形モデル\n",
    "---\n",
    "　入力が$1$次元（入力が$1$つ）の場合，関数$f$（学習対象の関数）をデータから近似するための一番単純なモデルは，入力に関する線形モデル$\\theta×x$である．ここで，$\\theta$はモデルのパラメータ（スカラー，重み）を表し，このパラメータを学習することにより関数を近似する．しかし，このモデルはパラメータ$\\theta$に関して線形で数学的に扱いやすいが，直線関数しか表現できないため，表現力が乏しく実用的でない．<br>\n",
    " 　そこで，このモデルが非線形（ぐにょぐにょ曲がってるような曲線）の入出力関数も表現できるように拡張したものが，パラメータに関する線形モデルである．（非線形なのに線形モデルってどゆこと？）<br>\n",
    "\\begin{equation}\n",
    "f_{\\theta}(x)=\\sum_{j=1}^{b}\\phi_{j}(\\theta_{j}x)=\\sum_{j=1}^{b}\\theta_{j}\\phi_{j}(x)=\\boldsymbol{\\theta}^{T}\\phi(x)\n",
    "\\end{equation}\n",
    "　ここで，$\\phi_{j}(x)$は**基底関数**ベクトル$\\phi(x)=(\\phi_{1}(x)...\\phi_{b}(x))^{T}$の第$j$要素を，$\\theta_{j}$はパラメータベクトル$\\theta=(\\theta_{1}...\\theta_{b})^{T}$の第$j$要素を返す．$b$は基底関数の数を，$^{T}$は転置を表す．このモデルはパラメータベクトル$\\theta$に関しては線形なままだが，例えば，基底関数として**多項式**\n",
    "\\begin{equation}\n",
    "    \\phi(x)=(1,x,x^{2},...,x^{b-1})^{T}\n",
    "\\end{equation}\n",
    "や，$b=2m+1$に対して**三角多項式**\n",
    "\\begin{equation}\n",
    "    \\phi(x)=(1,sinx,cosx,...,sinmx,cosmx)^{T}\n",
    "\\end{equation}\n",
    "などを選ぶことにより，複雑な非線形関数を表現できる．<br>\n",
    "　このモデルは，$d$次元の入力ベクトル$x=(x^{(1)},...,x^{(d)})^{T}$にも拡張できる．\n",
    "\\begin{equation}\n",
    "    f_{\\theta}(x)=\\sum_{j=1}^{b}\\theta_{j}\\phi_{j}(\\boldsymbol{x})=\\boldsymbol{\\theta}^{T}\\phi(\\boldsymbol{x})\n",
    "\\end{equation}\n",
    "多次元の入力ベクトル（入力が複数）$\\boldsymbol{x}$に対しては基底関数をどのように選べばよいのか？<br>\n",
    "→$1$次元の基底関数を持ちいて多次元の基底関数を構成する乗法モデルと加法モデルの紹介<br>\n",
    "　**乗法モデル**では，$1$次元の基底関数の積によって多次元の基底関数を構成する．\n",
    "\\begin{equation}\n",
    "    f_{\\theta}(x)=\\sum_{j_{1}=1}^{b'}\\theta_{j_{1}}\\phi_{j_{1}}(x^{(1)})\\sum_{j_{2}=1}^{b'}\\theta_{j_{2}}\\phi_{j_{2}}(x^{(2)})…\\sum_{j_{d}=1}^{b'}\\theta_{j_{d}}\\phi_{j_{d}}(x^{(d)})\\\\\n",
    "    =\\sum_{j_{1}=1}^{b'}\\sum_{j_{2}=1}^{b'}…\\sum_{j_{d}=1}^{b'}\\theta_{j_{1}}\\phi_{j_{1}}(x^{(1)})\\theta_{j_{2}}\\phi_{j_{2}}(x^{(2)})…\\theta_{j_{d}}\\phi_{j_{d}}(x^{(d)})\\\\\n",
    "    =\\sum_{j_{1}=1}^{b'}\\sum_{j_{2}=1}^{b'}…\\sum_{j_{d}=1}^{b'}\\theta_{j_{1},j_{2},...,j_{d}}\\phi_{j_{1}}(x^{(1)})\\phi_{j_{2}}(x^{(2)})…\\phi_{j_{d}}(x^{(d)})\n",
    "\\end{equation}\n",
    "ここで，$b'$は各次元のパラメータ（基底関数と重みの数）を表している．乗法モデルでは**全ての$1$次元の基底関数の組み合わせを考えるので複雑な関数を表現できる**が，全体のパラメータ数は$(b')^{d}$となり，入力次元$d$に対して指数関数的に増加してしまう．例えば，$b'=10$に対して入力次元$d$が100のとき，全体のパラメータ数は\n",
    "\\begin{equation}\n",
    "    10^{100}=1000…000(0が100個)\n",
    "\\end{equation}\n",
    "と天文学的数字になってしまう．このような次元数に対してパラメータ数が指数的に増加することは**次元の呪い**と呼ばれており，機械学習の研究者はまじぱない卍と思っている．つまり，入力の次元数が増えると，学習の難しさが指数的に増加するということ．この「次元の呪い」からいかにして逃れるかが大事である．<br>\n",
    "　**加法モデル**では，$1$次元の基底関数の和によって多次元の基底関数を構成する．\n",
    "\\begin{equation}\n",
    "    f_{\\theta}(\\boldsymbol{x})=\\sum_{j=1}^{b'}\\theta_{1,j}\\phi_{j}(x^{(1)}) + … +  \\theta_{d,j}\\phi_{j}(x^{(d)}) \n",
    "    =\\sum_{k=1}^{d}\\sum_{j=1}^{b'}\\theta_{k,j}\\phi_{j}(x^{(k)})\n",
    "\\end{equation}\n",
    "加法モデルでは全体のパラメータ数は$b'd$となり，入力次元$d$に対して線形にしか増加しない．例えば，$b'=10$に対して入力次元$d$が$100$のとき，全体のパラメータ数は$10×100=1000$となる．しかし，加法モデルは**$1$次元の基底関数の和しか考えないため，乗法モデルほど複雑な関数を表現できない**．\n",
    "![乗法・加法モデル](jouhou_kahou.PNG　\"乗法・加法モデルの表現力について\")\n",
    "Fig. 乗法・加法モデルの表現力（乗法モデルの方が表現力があるが，パラメータ数が多い，反対に加法モデルの方が表現力が少ないが，パラメータ数は少なくて済む）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 カーネルモデル\n",
    "---\n",
    "　線形モデルでは，基底関数を多項式や三角多項式など訓練標本（学習データ）${(x_{i},y_{i})}_{i=1}^{n}$とは無関係に決めていた．ここでは，**基底関数の設計**に入力標本${x_{i}}_{i=1}^{n}$を利用する**カーネルモデル**について説明する．<br>\n",
    "     カーネルモデルは，$K(・,・)$で表される**カーネル関数**と呼ばれる二変数関数を用いて，${K(x,x_{j})}_{j=1}^{n}$の線形結合として定義される．\n",
    "\\begin{equation}\n",
    "    f_{\\theta}(x)=\\sum_{j=1}^{n}\\theta_{k}K(x,x_{j})\n",
    "\\end{equation}\n",
    "カーネル関数としては，例えば，**ガウスカーネル**が良く用いられます．\n",
    "\\begin{equation}\n",
    "    K(x, c) = exp(-\\frac{\\|x-c\\|^{2}}{2h^2})\n",
    "\\end{equation}\n",
    "ここで，$\\|・\\|$は**$l_{2}$ノルム**（二乗和の平方根・ユークリッド距離）$\\|x\\|=\\sqrt{x^{T}x}$を表す（ノルムとは，ベクトル空間上の距離のこと）．$h$と$c$はそれぞれガウスカーネルの**バンド幅**と**中心**を表す．（$Lp$ノルムの説明は，wikipediaの「有限次元の$lp$ノルム」の説明が分かりやすい https://ja.wikipedia.org/wiki/Lp%E7%A9%BA%E9%96%93 ）<br>\n",
    "　ガウスカーネルモデルでは，各入力標本$\\{x_{i}\\}_{i=1}^{n}$にガウスカーネルを配置し，それらの高さ$\\{\\theta_{i}\\}_{i=1}^{n}$をパラメータとして学習します．従って，ガウスカーネルモデルでは，訓練入力標本のある場所の近傍のみで関数を近似することになる．乗法モデルでは，入力空間全体で関数を近似しているのに対して，**ガウスカーネルモデルでは入力標本$\\{x_{i}\\}_{i=1}^{n}$の近傍でのみ関数を近似することによって，次元の呪いの影響を軽減**しようとしている．\n",
    "![1次元のガウスカーネルモデル](1次元のガウスカーネルモデル.PNG　\"1次元のガウスカーネルモデル\")\n",
    "Fig. 1次元のガウスカーネルモデル（各入力標本$\\{x_{i}\\}_{i=1}^{n}$にガウスカーネルを配置し，その高さをパラメータ$\\{\\theta_{i}\\}_{i=1}^{n}$として学習する\n",
    "![2次元のガウスカーネルモデル](2次元のガウスカーネルモデル.PNG　\"2次元のガウスカーネルモデル\")\n",
    "　　　Fig. 2次元のガウスカーネルモデル（入力変数$x$の次元数が増えてもカーネルの数は訓練標本数$n$で決まる）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
